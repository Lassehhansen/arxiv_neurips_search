{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Searching for specific claims that AUPRC is suprior to AUROC with GPT 3.5/4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining prompt and importing function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in machine learning and scientific literature review.\n",
    "For each chunk of a published paper (which may have typos, misspellings, and odd characters as a result of conversion from PDF), return a JSON object that states whether or not the paper makes any claim that the area under the precision recall curve (AUPRC) is superior or inferior as a general performance metric to the area under the receiver operating characteristic (AUROC) in an ML setting, in particular for imbalanced classification problems. A paper claiming that a model performs better under AUPRC vs. AUROC is *not* an example of this; instead a paper claiming that AUPRC should be used instead of AUROC in cases of class imbalance is an example of this metric commentary. Respond with format {\"claims\": [{\"claim\": DESCRIPTION OF CLAIM, \"evidence_quote\": SUBSTRING FROM INPUT STATING CLAIM}, ...]}. If the paper makes no claims, leave the \"claims\" key in the JSON object empty. If the claim made is that the AUPRC is superior to the AUROC in the case of class imbalance, use the string \"AUPRC is superior to AUROC for imbalanced data\" for the description of the claim. For other claims, use any appropriate free-text description.\n",
    "\n",
    "Examples: \n",
    "\n",
    "Input: \"AUROC: The horizontal and vertical coordinates of the Receiver Operating Characteristic (ROC) curve are the FPR and TPR, and the curve is obtained by calculating the FPR and TPR under multiple sets of thresholds. The area of the region enclosed by the ROC curve and the horizontal axis is often used to evaluate binary classification tasks, denoted as AUROC. The value of AUROC is within the range of [0, 1], and higher values indicate better performance. AUROC can visualize the generalization performance of the GVAED model and help to select the best alarm threshold In addition, the Equal Error Rate (EER), i.e., the proportion of incorrectly classified frames when TPR and FNR are equal, is also used to measure the performance of anomaly detection models. AP: Due to the highly unbalanced nature of positive and negative samples in GVAED tasks, i.e., the TN is usually larger than the TP, researchers think that the area under the Precision-Recall (PR) curve is more suitable for evaluating GVAED models, denoted as AP. The horizontal coordinates of the PR curve are the Recall (i.e., the TPR in Eq. 4), while the vertical coordinate represents the Precision, defined as Precision = TP TP+FP . A point on the PR curve corresponds to the Precision and Recall values at a certain threshold.\"\n",
    "Output: {\"claims\": [{\"claim\": \"AUPRC is superior to AUROC for imbalanced data\", \"evidence_quote\": \"Due to the highly unbalanced nature of positive and negative samples in GVAED tasks, i.e., the TN is usually larger than the TP, researchers think that the area under the Precision-Recall (PR) curve is more suitable for evaluating GVAED models, denoted as AP\"}]}\n",
    "\n",
    "Input: \"As seen, it outperforms other approaches except in the cases of TinyImageNet for CIFAR-100. Our approach still has better AUROC, but the detection error and FPR at 95% TPR are slightly larger than ODIN’s. Interestingly, the MD approach is worse than max-softmax in some cases. Such a result has also been reporte\"\n",
    "Output: {\"claims\": []}\n",
    "\n",
    "Input: \"AUC-ROC measures the class separability at various threshold settings. ROC is the probability curve and AUC represents the degree of measures of separability. It compares true positive rate (sensitivity/recall) versus the false positive rate (1 - specificity). The higher the AUC-ROC, the bigger the distinction between the true positive and false negative. • AUC-PR: It combines the precision and recall, for various threshold values, it compares the positively predicted value (precision) vs the true positive rate (recall). Both precision and recall focus on the positive class (the lesion) and unconcerned about the true negative (not a lesion, which is the majority class). Thus, for class imbalance, PR is more suitable than ROC. The higher the AUC-PR, the better the model performance\"\n",
    "Output: {\"claims\": [{\"claim\": \"AUPRC is superior to AUROC for imbalanced data\", \"evidence_quote\": \"Thus, for class imbalance, PR is more suitable than ROC\"}]}\n",
    "\n",
    "So please for each chunk of the input, return a JSON object that states whether or not the paper makes any claim that the area under the precision recall curve (AUPRC) is superior or inferior as a general performance metric to the area under the receiver operating characteristic (AUROC) in an ML setting, in particular for imbalanced classification problems. \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load df with context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df_with_context_windows = pd.read_csv('data/filtered_data_with_context_windows_v4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up the environment\n",
    "import sys\n",
    "import os\n",
    "sys.path.append('src')  # Adjust this path to ensure it points to the correct directory\n",
    "from claim_search_v3 import process_all_context_windows\n",
    "\n",
    "# Set your OpenAI API key and other relevant parameters\n",
    "model = \"gpt-3.5-turbo-1106\"\n",
    "openai_api_key = \"INSERT API KEY HERE\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the context windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 1000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 2000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 3000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 4000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 5000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 6000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 7000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 8000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 9000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 10000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 11000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 12000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 13000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 14000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 15000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 16000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 17000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 18000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 19000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 20000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 21000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 22000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 23000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 24000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 25000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 26000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 27000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 28000/29498 texts; pausing for 0.5 seconds...\n",
      "Processed 29000/29498 texts; pausing for 0.5 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Process the DataFrame\n",
    "df_claims = process_all_context_windows(df_with_context_windows, \n",
    "                                        model, \n",
    "                                        SYSTEM_PROMPT, \n",
    "                                        openai_api_key, \n",
    "                                        texts_before_pause=1000, \n",
    "                                        pause_duration=0.5, \n",
    "                                        max_workers = 6)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df_claims.to_csv('data/processed_gpt_responses_total_run_v2.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extract the claims from the json format in the column \"gpt_response\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/rm/48r5vtns6fz6xtmhbq04sd7d03b5vb/T/ipykernel_27820/2795003071.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_filtered_claims_2['claim'] = df_filtered_claims_2['gpt_response'].apply(extract_claim)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "# Define function to extract claims\n",
    "def extract_claim(row):\n",
    "    try:\n",
    "        data = json.loads(row)\n",
    "        claims_for_row = []\n",
    "        if 'claims' in data:  # Check if 'claims' key is in the dictionary\n",
    "            for claim_dict in data['claims']:  # Iterate over each claim\n",
    "                if isinstance(claim_dict, dict) and 'claim' in claim_dict:  # Check if it's a dictionary and has 'claim'\n",
    "                    claims_for_row.append(claim_dict['claim'])\n",
    "        return \" | \".join(claims_for_row)\n",
    "    except json.JSONDecodeError:\n",
    "        return None  # Return None or an empty string '' if preferred\n",
    "    except TypeError:\n",
    "        return None  # Handle cases where the row might not be properly formatted\n",
    "\n",
    "# Apply the function to each row in the 'gpt_response' column to create the new 'claim' column\n",
    "df_filtered_claims_2['claim'] = df_filtered_claims_2['gpt_response'].apply(extract_claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find unique claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_claims_array = df_filtered_claims_2['claim'].unique()\n",
    "unique_claims_df = pd.DataFrame(unique_claims_array, columns=['Unique Claims'])\n",
    "unique_claims_df.to_csv('claims.csv', index=False, sep=',', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter out only the claims we are interested in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of claims we want to keep\n",
    "claims_to_keep = [\n",
    "    \"AUC is a common but worse measure than AP\",\n",
    "    \"AUPRC is more sensitive than AUC in skewed data like CVR prediction task\",\n",
    "    \"AUPRC is superior to AUROC for imbalanced data\",\n",
    "    \"PR-AUC is more robust in the face of imbalanced data\",\n",
    "    \"AUPRC is more suitable than AUROC for evaluating models\",\n",
    "    \"AUPRC is not dependent on the choice of a specific threshold\"\n",
    "]\n",
    "\n",
    "# Filter out the DataFrame rows where 'claim' is not in the claims_to_keep list\n",
    "df_filtered_claims_3 = df_filtered_claims_2[df_filtered_claims_2['claim'].isin(claims_to_keep)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save the new filtered claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_claims_3.to_csv('data/filtered_claims_new_v4.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GPT 4 Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Redefining systemt prompt, that does not have the last message from before. This is because we add such a description in the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"\n",
    "You are an expert in machine learning and scientific literature review.\n",
    "For each chunk of a published paper (which may have typos, misspellings, and odd characters as a result of conversion from PDF), return a JSON object that states whether or not the paper makes any claim that the area under the precision recall curve (AUPRC) is superior or inferior as a general performance metric to the area under the receiver operating characteristic (AUROC) in an ML setting, in particular for imbalanced classification problems. A paper claiming that a model performs better under AUPRC vs. AUROC is *not* an example of this; instead a paper claiming that AUPRC should be used instead of AUROC in cases of class imbalance is an example of this metric commentary. Respond with format {\"claims\": [{\"claim\": DESCRIPTION OF CLAIM, \"evidence_quote\": SUBSTRING FROM INPUT STATING CLAIM}, ...]}. If the paper makes no claims, leave the \"claims\" key in the JSON object empty. If the claim made is that the AUPRC is superior to the AUROC in the case of class imbalance, use the string \"AUPRC is superior to AUROC for imbalanced data\" for the description of the claim. For other claims, use any appropriate free-text description.\n",
    "\n",
    "Examples: \n",
    "\n",
    "Input: \"AUROC: The horizontal and vertical coordinates of the Receiver Operating Characteristic (ROC) curve are the FPR and TPR, and the curve is obtained by calculating the FPR and TPR under multiple sets of thresholds. The area of the region enclosed by the ROC curve and the horizontal axis is often used to evaluate binary classification tasks, denoted as AUROC. The value of AUROC is within the range of [0, 1], and higher values indicate better performance. AUROC can visualize the generalization performance of the GVAED model and help to select the best alarm threshold In addition, the Equal Error Rate (EER), i.e., the proportion of incorrectly classified frames when TPR and FNR are equal, is also used to measure the performance of anomaly detection models. AP: Due to the highly unbalanced nature of positive and negative samples in GVAED tasks, i.e., the TN is usually larger than the TP, researchers think that the area under the Precision-Recall (PR) curve is more suitable for evaluating GVAED models, denoted as AP. The horizontal coordinates of the PR curve are the Recall (i.e., the TPR in Eq. 4), while the vertical coordinate represents the Precision, defined as Precision = TP TP+FP . A point on the PR curve corresponds to the Precision and Recall values at a certain threshold.\"\n",
    "Output: {\"claims\": [{\"claim\": \"AUPRC is superior to AUROC for imbalanced data\", \"evidence_quote\": \"Due to the highly unbalanced nature of positive and negative samples in GVAED tasks, i.e., the TN is usually larger than the TP, researchers think that the area under the Precision-Recall (PR) curve is more suitable for evaluating GVAED models, denoted as AP\"}]}\n",
    "\n",
    "Input: \"As seen, it outperforms other approaches except in the cases of TinyImageNet for CIFAR-100. Our approach still has better AUROC, but the detection error and FPR at 95% TPR are slightly larger than ODIN’s. Interestingly, the MD approach is worse than max-softmax in some cases. Such a result has also been reporte\"\n",
    "Output: {\"claims\": []}\n",
    "\n",
    "Input: \"AUC-ROC measures the class separability at various threshold settings. ROC is the probability curve and AUC represents the degree of measures of separability. It compares true positive rate (sensitivity/recall) versus the false positive rate (1 - specificity). The higher the AUC-ROC, the bigger the distinction between the true positive and false negative. • AUC-PR: It combines the precision and recall, for various threshold values, it compares the positively predicted value (precision) vs the true positive rate (recall). Both precision and recall focus on the positive class (the lesion) and unconcerned about the true negative (not a lesion, which is the majority class). Thus, for class imbalance, PR is more suitable than ROC. The higher the AUC-PR, the better the model performance\"\n",
    "Output: {\"claims\": [{\"claim\": \"AUPRC is superior to AUROC for imbalanced data\", \"evidence_quote\": \"Thus, for class imbalance, PR is more suitable than ROC\"}]}\n",
    "\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining introduction and end statement to the user prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "introduction_statement_prompt = \"\"\"\n",
    "Please carefully review the following text. We are specifically looking for claims where AUPRC is argued to be a superior metric to AUROC, especially in cases of class imbalance in machine learning applications. Any claim that discusses the preference of AUPRC over AUROC due to its effectiveness in such scenarios should be returned in the a JSON object. If no such claims are found, please leave the 'claims' key empty. Here is the text:\n",
    "\"\"\"\n",
    "end_statement_prompt = \"\"\"\n",
    "If you find any claim asserting the superiority of AUPRC over AUROC for imbalanced datasets, please provide your findings in a JSON object with the key 'claims'. Each claim should be a dictionary with 'claim' and 'evidence_quote' as keys, like this: {\"claims\": [{\"claim\": \"DESCRIPTION OF CLAIM\", \"evidence_quote\": \"SUBSTRING FROM INPUT STATING CLAIM\"}]}. If no relevant claims are found, the 'claims' key should have an empty list.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('src')  # Adjust this path to ensure it points to the correct directory\n",
    "\n",
    "from claim_search_v4 import process_all_context_windows\n",
    "\n",
    "model = \"gpt-4-0125-preview\"\n",
    "openai_api_key = \"INSERT API KEY HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/2000 texts; pausing for 0.5 seconds...\n",
      "Processed 1000/2000 texts; pausing for 0.5 seconds...\n",
      "Processed 1500/2000 texts; pausing for 0.5 seconds...\n",
      "Processed 2000/2000 texts; pausing for 0.5 seconds...\n"
     ]
    }
   ],
   "source": [
    "# Running the first 2000 rows, to make sure new setup is efficient\n",
    "df_4_0_first_2000 = df_filtered_claims_3.iloc[:2000]\n",
    "df_claims_4_0_first_2000 = process_all_context_windows(df_4_0_first_2000, model, SYSTEM_PROMPT, introduction_statement_prompt, end_statement_prompt, openai_api_key, texts_before_pause=500, pause_duration=0.5, max_workers=6)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df_claims_4_0_first_2000.to_csv('data/processed_gpt_4_0_responses_total_run_v2_first_2000.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Works as intended, as such we are running the rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 1000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 1500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 2000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 2500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 3000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 3500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 4000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 4500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 5000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 5500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 6000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 6500/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 7000/7591 texts; pausing for 0.5 seconds...\n",
      "Processed 7500/7591 texts; pausing for 0.5 seconds...\n"
     ]
    }
   ],
   "source": [
    "df_4_0_rest = df_filtered_claims_3.iloc[2000:]\n",
    "df_claims_4_0_rest = process_all_context_windows(df_4_0_rest, model, SYSTEM_PROMPT, introduction_statement_prompt, end_statement_prompt, openai_api_key, texts_before_pause=500, pause_duration=0.5, max_workers=6)\n",
    "\n",
    "# Save the updated DataFrame\n",
    "df_claims_4_0_rest.to_csv('data/processed_gpt_4_0_responses_total_run_v2_rest.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combining the two dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_4_0 = pd.concat([df_claims_4_0_first_2000, df_claims_4_0_rest])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting claims with previously defined function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the function to each row in the 'gpt_response' column to create the new 'claim' column\n",
    "combined_df_4_0['claim'] = combined_df_4_0_no_non_clams['gpt_response'].apply(extract_claim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving a csv file with claims and the number of times they are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "unique_claims_array_4_0 = combined_df_4_0_no_non_clams['claim'].unique()\n",
    "unique_claims_df_4_0 = pd.DataFrame(unique_claims_array_4_0, columns=['Unique Claims'])\n",
    "\n",
    "# Counting occurrences of each claim\n",
    "claim_counts = combined_df_4_0_no_non_clams['claim'].value_counts().reset_index()\n",
    "claim_counts.columns = ['Unique Claims', 'Count']\n",
    "\n",
    "# Merging\n",
    "unique_claims_df_4_0_with_counts = pd.merge(unique_claims_df_4_0, claim_counts, on='Unique Claims', how='left')\n",
    "\n",
    "# Saving to CSV\n",
    "unique_claims_df_4_0_with_counts.to_csv('claims_4_0_with_counts.csv', index=False, sep=',', quoting=csv.QUOTE_ALL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving only relvevant claims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of claims we want to keep\n",
    "claims_to_keep = [\n",
    "\"AUPRC is superior to AUROC for imbalanced data\",\n",
    "\"AP may be considered a more attractive performance metric than AUC\",\n",
    "\"AUPRC may be considered a more attractive performance metric than AUROC\",\n",
    "\"AUPRC is superior to AUROC for imbalanced data | AUPRC is superior to AUROC for imbalanced data\",\n",
    "\"AP is a better metric for discriminates the risk prediction performance than the AUC does\",\n",
    "\"AUPRC might be preferred for datasets with known proportion of anomalies\"\n",
    "]\n",
    "\n",
    "# Filter out the DataFrame rows where 'claim' is not in the claims_to_keep list\n",
    "combined_df_4_0_filtered = combined_df_4_0[combined_df_4_0['claim'].isin(claims_to_keep)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving the final claims, so they can be manually annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df_4_0_filtered.to_csv('data/filtered_claims_4_0_new_v4_final.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
